{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:48.043609Z",
     "start_time": "2025-05-15T18:15:47.516059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from distributed.diagnostics.progress_stream import counts\n",
    "from roboflow import Roboflow\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:48.410676Z",
     "start_time": "2025-05-15T18:15:48.046415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf = Roboflow(api_key=\"*\")\n",
    "\n",
    "\n"
   ],
   "id": "a629b0f55d856ec7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:48.835836Z",
     "start_time": "2025-05-15T18:15:48.442526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "workspace = rf.workspace()             # aktif workspace bilgisi\n",
    "print(workspace.list_projects()) \n"
   ],
   "id": "cd728b8ad178d4db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "[{'id': 'taa-ewhrv/line-detection-vylwz', 'type': 'object-detection', 'name': 'Line Detection', 'created': 1746702405.51, 'updated': 1746959795.923, 'images': 121, 'unannotated': 0, 'annotation': 'Line', 'versions': 1, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'trainParsed': '91', 'test': 10, 'validPercent': '17', 'trainPercent': '75', 'valid': 21, 'validParsed': '20', 'testPercent': '8', 'train': 90, 'testParsed': '10'}, 'colors': {'Line': '#C7FC00'}, 'classes': {'Line': 1340}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/thumb.jpg', 'annotation': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/annotation-Line.png'}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'shear': {'horizontal': 10, 'vertical': 10}, 'rotate': {'degrees': 15}, 'brightness': {'percent': 16, 'brighten': True, 'darken': True}, 'noise': {'percent': 0.18}}}, {'id': 'taa-ewhrv/my-first-project-t3aft', 'type': 'object-detection', 'name': 'My First Project', 'created': 1744636006.583, 'updated': 1745655254.669, 'images': 7, 'unannotated': 0, 'annotation': 'Harf', 'versions': 2, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'train': 4, 'valid': 2, 'test': 1, 'export': 0}, 'colors': {'Harf': '#C7FC00', 'A': '#8622FF', 'B': '#FE0056', 'C': '#00FFCE', 'D': '#FF8000', 'E': '#00B7EB', 'F': '#FFFF00', 'G': '#FF00FF', 'I': '#0E7AFE', 'J': '#FFABAB', 'K': '#0000FF', 'L': '#a0522d', 'M': '#808000', 'n': '#483d8b', 'O': '#8b008b', 'P': '#ff4500', 'R': '#dc143c', 'S': '#00ffff', 'T': '#d8bfd8', 'U': '#ff1493', 'V': '#ffe4b5', 'Y': '#db7093', 'Z': '#deb887', 'H': '#6495ed'}, 'classes': {'J': 5, 'F': 17, 'Harf': 15, 'V': 13, 'P': 13, 'Z': 13, 'D': 27, 'B': 25, 'G': 19, 'T': 30, 'E': 47, 'H': 8, 'C': 18, 'O': 17, 'n': 38, 'U': 26, 'Y': 21, 'R': 23, 'M': 46, 'I': 52, 'K': 28, 'A': 89, 'S': 33, 'L': 37}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/thumb.jpg', 'annotation': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/annotation-Harf.png'}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'rotate': {'degrees': 11}, 'crop': {'min': 0, 'percent': 26}, 'shear': {'horizontal': 10, 'vertical': 10}}}, {'id': 'taa-ewhrv/text-recognation-svukt', 'type': 'classification', 'name': 'Text Recognation', 'created': 1746702986.492, 'updated': 1747213785.908, 'images': 609, 'unannotated': 0, 'annotation': 'Taha-Meryem-Elif-Berat-Ruhan', 'versions': 1, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'export': 0, 'valid': 124, 'test': 60, 'train': 425}, 'colors': {'Unlabeled': '#C7FC00', 'taha': '#8622FF', 'Elif': '#FE0056', 'Meryem Ece AYYILDIZ': '#00FFCE', 'Berat': '#FF8000', 'rana': '#00B7EB'}, 'classes': {'Unlabeled': 0, 'taha': 133, 'Elif': 188, 'Meryem Ece AYYILDIZ': 99, 'Berat': 99, 'rana': 90}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/zhXOtUKU4N14vu2S9qwq/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/zhXOtUKU4N14vu2S9qwq/thumb.jpg', 'annotation': None}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'crop': {'min': 0, 'percent': 20}, 'exposure': {'percent': 10}, 'noise': {'percent': 0.1}, 'brightness': {'percent': 15, 'brighten': True, 'darken': True}}}]\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:52.643653Z",
     "start_time": "2025-05-15T18:15:48.850027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project = rf.workspace(\"taa-ewhrv\").project(\"line-detection-vylwz\")  # Workspace + Project slug\n",
    "model = project.version(1).model\n",
    "project2 = rf.workspace(\"taa-ewhrv\").project(\"text-recognation-svukt\") \n",
    "model2 = project2.version(1).model"
   ],
   "id": "c1dcef8f1ba137dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:55.654583Z",
     "start_time": "2025-05-15T18:15:52.656333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = model.predict(\"deneme.png\").json()\n",
    "\n",
    "print(prediction)"
   ],
   "id": "44440cddda294921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'x': 207, 'y': 730, 'width': 143, 'height': 69, 'confidence': 0.8106053471565247, 'class': 'Line', 'class_id': 0, 'detection_id': 'f56b1aa2-ff94-4b1f-9e1f-263e0b34f846', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 593, 'y': 472, 'width': 923, 'height': 82, 'confidence': 0.7945687770843506, 'class': 'Line', 'class_id': 0, 'detection_id': '8a113fad-0ecf-4371-91ea-16752be7ff51', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 637, 'y': 379, 'width': 1022, 'height': 82, 'confidence': 0.7915651798248291, 'class': 'Line', 'class_id': 0, 'detection_id': '29b5e5da-dc50-4d5c-87f8-9043687d9bb0', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 480, 'y': 817, 'width': 656, 'height': 85, 'confidence': 0.7896133065223694, 'class': 'Line', 'class_id': 0, 'detection_id': '40b5460c-2aed-46a5-9e1d-b496a0e04127', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 612, 'y': 571, 'width': 944, 'height': 87, 'confidence': 0.7832475304603577, 'class': 'Line', 'class_id': 0, 'detection_id': '5bcaac3d-4ce4-4329-914b-b91f90edd846', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 610, 'y': 970, 'width': 793, 'height': 95, 'confidence': 0.7740350961685181, 'class': 'Line', 'class_id': 0, 'detection_id': '0a35ea84-e998-4e68-856c-1052b396d9c0', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 567, 'y': 286, 'width': 898, 'height': 93, 'confidence': 0.7727227807044983, 'class': 'Line', 'class_id': 0, 'detection_id': '33f55f50-fe15-4fa8-92c3-c2f0368de953', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 631, 'y': 657, 'width': 975, 'height': 90, 'confidence': 0.7656806707382202, 'class': 'Line', 'class_id': 0, 'detection_id': '84ae759f-ffe1-4008-9732-98442fb3c5b5', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}], 'image': {'width': '1244', 'height': '1654'}}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:15:55.672938Z",
     "start_time": "2025-05-15T18:15:55.670408Z"
    }
   },
   "cell_type": "code",
   "source": "from PIL import Image",
   "id": "7eaeb17c084526f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:03.019597Z",
     "start_time": "2025-05-15T18:15:55.679419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sympy import false\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n"
   ],
   "id": "cb6affcf3a58d6f1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:03.029215Z",
     "start_time": "2025-05-15T18:16:03.027632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import os\n",
    "import uuid"
   ],
   "id": "c6369448260989bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:05.129129Z",
     "start_time": "2025-05-15T18:16:03.036313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten', use_fast=False)\n",
    "model_mic = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')"
   ],
   "id": "b07910653aca938a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:05.277887Z",
     "start_time": "2025-05-15T18:16:05.238022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "from flask_cors import CORS, cross_origin\n",
    "from io import BytesIO\n",
    "\n"
   ],
   "id": "e9d1ff1cab79d6e9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:05.364413Z",
     "start_time": "2025-05-15T18:16:05.361614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "UPLOAD_FOLDER = 'uploads'\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n"
   ],
   "id": "188dd764d12ddcb3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:05.458719Z",
     "start_time": "2025-05-15T18:16:05.451172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@app.route(\"/predict\" , methods=['POST'])\n",
    "@cross_origin()\n",
    "def predict():\n",
    "    try :\n",
    "        #!!!!!!!Ayarlanacak\n",
    "        \n",
    "        image_file = request.files['image']\n",
    "        #image0 = Image.open(image_file.stream).convert('RGB')\n",
    "        \n",
    "        filename = f\"{uuid.uuid4().hex}.png\"\n",
    "        filepath = os.path.join(UPLOAD_FOLDER, filename)\n",
    "        \n",
    "        image = Image.open(image_file.stream).convert('RGB')\n",
    "        image.save(filepath)\n",
    "        \n",
    "        \n",
    "        \n",
    "        image0=Image.open(filepath)\n",
    "        print(\"log1\")\n",
    "        \n",
    "        prediction = model.predict(filepath).json()\n",
    "        predictions = prediction[\"predictions\"]\n",
    "        image = image0\n",
    "        print(\"log2\")\n",
    "    \n",
    "        auth=\"\"\n",
    "        count=len(predictions)+1\n",
    "        #predictions =predictions.reverse();\n",
    "        results = []\n",
    "        for i, pred in enumerate(predictions):\n",
    "            print(\"log3\")\n",
    "            count-=1;\n",
    "            x = pred['x']\n",
    "            y = pred['y']\n",
    "            w = pred['width']\n",
    "            h = pred['height']\n",
    "            \n",
    "            # (x, y) merkezden k√∂≈üe koordinatlarƒ±na √ßevir\n",
    "            left = int(x - w / 2)\n",
    "            top = int(y - h / 2)\n",
    "            right = int(x + w / 2)\n",
    "            bottom = int(y + h / 2)\n",
    "        \n",
    "            # G√∂rseli kƒ±rp\n",
    "            cropped = image.crop((left, top, right, bottom))\n",
    "            #cropped.show()\n",
    "            cropped = cropped.resize((cropped.width * 2, cropped.height * 2), Image.LANCZOS)\n",
    "            temp_cropped = cropped.convert(\"RGB\")\n",
    "            \n",
    "            #ge√ßici diske kayƒ±t\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".jpg\") as temp:\n",
    "                temp_cropped.save(temp.name)  # PIL Image'ƒ± diske kaydet\n",
    "                prediction2 = model2.predict(temp.name)\n",
    "                predictions2 = prediction2[0]\n",
    "                predictionDENEME = predictions2.json();\n",
    "                DENEME =predictionDENEME[\"top\"]\n",
    "                #print(DENEME)\n",
    "                #print(type(prediction2))\n",
    "                #print(dir(prediction2))\n",
    "                #print(type(prediction2[0]))\n",
    "                #print(dir(prediction2[0]))\n",
    "                auth = DENEME\n",
    "        \n",
    "        \n",
    "            #deneme_rgb=cropped.convert('RGB')\n",
    "            \n",
    "            image_array = np.array(cropped)\n",
    "            \n",
    "            \n",
    "            grayImage = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "            # Otsu Thresholding uygula\n",
    "            # Otsu, e≈üik deƒüerini otomatik olarak belirler\n",
    "            \n",
    "            adaptive_thresh = cv2.adaptiveThreshold(\n",
    "            grayImage, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,\n",
    "            21,  # blockSize (daima tek sayƒ±, 11-15 arasƒ± genelde iyi)\n",
    "            4   # C deƒüeri (0 ya da negatif dene)\n",
    "        )\n",
    "            \n",
    "            ret, otsu_thresh = cv2.threshold(grayImage, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "            # Morfolojik i≈ülemler - harflerdeki kopmalarƒ± birle≈ütirmek i√ßin\n",
    "            # 1. Kapama (Closing) i≈ülemi - kopuk harfleri birle≈ütirir\n",
    "            kernel = np.ones((2, 2), np.uint8)  # Kernel boyutunu metninize g√∂re ayarlayabilirsiniz\n",
    "            closing = cv2.morphologyEx(otsu_thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "            # 2. Geni≈ületme (Dilation) i≈ülemi - harfleri daha kalƒ±n yapar, bo≈üluklarƒ± doldurur\n",
    "            dilation_kernel = np.ones((1, 1), np.uint8)\n",
    "            dilated_image = cv2.dilate(closing, dilation_kernel, iterations=1)\n",
    "        \n",
    "            # 3. Median filtreleme - g√ºr√ºlt√ºy√º azaltƒ±r\n",
    "            filtered_image = cv2.medianBlur(dilated_image, 3)\n",
    "        \n",
    "            # Numpy dizisini PIL Image'e d√∂n√º≈üt√ºr\n",
    "            threshold_image = Image.fromarray(otsu_thresh)\n",
    "            threshold_rgb = threshold_image.convert('RGB')\n",
    "            \n",
    "            last_image=Image.fromarray(adaptive_thresh)\n",
    "            last_img_rgb=last_image.convert('RGB')\n",
    "            #last_img_rgb.show()\n",
    "            \n",
    "            # G√∂r√ºnt√ºy√º g√∂ster\n",
    "            #threshold_rgb.show()\n",
    "            #deneme_rgb.show()\n",
    "        \n",
    "            # Modelle i≈ülem i√ßin g√∂r√ºnt√ºy√º hazƒ±rla\n",
    "            pixel_values = processor(images=threshold_rgb, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "            # Tahmin yap\n",
    "            generated_ids = model_mic.generate(pixel_values)\n",
    "            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            #print(f\"{count}. satƒ±r yazarƒ± : {auth}\")\n",
    "        \n",
    "            #print(\"\\nüñãÔ∏è Tanƒ±nan Metin:\\n\")\n",
    "            #print(generated_text)\n",
    "            \n",
    "            results.append({\n",
    "                \"predicted_text\": generated_text,\n",
    "                \"predicted_author\": auth\n",
    "            })\n",
    "            \n",
    "            \n",
    "        results.reverse()\n",
    "    \n",
    "        print(results)\n",
    "        return jsonify(results)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/hello\" , methods=['GET'])    \n",
    "@cross_origin()\n",
    "def hello() :\n",
    "    return \"Hello World!\"\n",
    "\n",
    "def run_app():\n",
    "    print(\"uyg ba≈üaldƒ± mƒ±\")\n",
    "    app.run(port=5002)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n",
    "    "
   ],
   "id": "432d800603a64ad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uyg ba≈üaldƒ± mƒ±\n",
      " * Serving Flask app '__main__'\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:16:05.548924Z",
     "start_time": "2025-05-15T18:16:05.547613Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8650fc3e0d6498d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
