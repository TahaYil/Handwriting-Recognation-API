{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:28.264522Z",
     "start_time": "2025-05-15T09:10:27.971072Z"
    }
   },
   "cell_type": "code",
   "source": "from roboflow import Roboflow\n",
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:28.775192Z",
     "start_time": "2025-05-15T09:10:28.268799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf = Roboflow(api_key=\"XtRZDk3yFHhs1VCx561P\")\n",
    "\n",
    "\n"
   ],
   "id": "a629b0f55d856ec7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:29.358149Z",
     "start_time": "2025-05-15T09:10:28.797166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "workspace = rf.workspace()             # aktif workspace bilgisi\n",
    "print(workspace.list_projects()) \n"
   ],
   "id": "cd728b8ad178d4db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "[{'id': 'taa-ewhrv/line-detection-vylwz', 'type': 'object-detection', 'name': 'Line Detection', 'created': 1746702405.51, 'updated': 1746959795.923, 'images': 121, 'unannotated': 0, 'annotation': 'Line', 'versions': 1, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'trainParsed': '91', 'test': 10, 'validPercent': '17', 'trainPercent': '75', 'valid': 21, 'validParsed': '20', 'testPercent': '8', 'train': 90, 'testParsed': '10'}, 'colors': {'Line': '#C7FC00'}, 'classes': {'Line': 1340}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/thumb.jpg', 'annotation': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/kGvEAE4KcDv4gHGC90QP/annotation-Line.png'}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'shear': {'horizontal': 10, 'vertical': 10}, 'rotate': {'degrees': 15}, 'brightness': {'percent': 16, 'brighten': True, 'darken': True}, 'noise': {'percent': 0.18}}}, {'id': 'taa-ewhrv/my-first-project-t3aft', 'type': 'object-detection', 'name': 'My First Project', 'created': 1744636006.583, 'updated': 1745655254.669, 'images': 7, 'unannotated': 0, 'annotation': 'Harf', 'versions': 2, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'train': 4, 'valid': 2, 'test': 1, 'export': 0}, 'colors': {'Harf': '#C7FC00', 'A': '#8622FF', 'B': '#FE0056', 'C': '#00FFCE', 'D': '#FF8000', 'E': '#00B7EB', 'F': '#FFFF00', 'G': '#FF00FF', 'I': '#0E7AFE', 'J': '#FFABAB', 'K': '#0000FF', 'L': '#a0522d', 'M': '#808000', 'n': '#483d8b', 'O': '#8b008b', 'P': '#ff4500', 'R': '#dc143c', 'S': '#00ffff', 'T': '#d8bfd8', 'U': '#ff1493', 'V': '#ffe4b5', 'Y': '#db7093', 'Z': '#deb887', 'H': '#6495ed'}, 'classes': {'J': 5, 'F': 17, 'Harf': 15, 'V': 13, 'P': 13, 'Z': 13, 'D': 27, 'B': 25, 'G': 19, 'T': 30, 'E': 47, 'H': 8, 'C': 18, 'O': 17, 'n': 38, 'U': 26, 'Y': 21, 'R': 23, 'M': 46, 'I': 52, 'K': 28, 'A': 89, 'S': 33, 'L': 37}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/thumb.jpg', 'annotation': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/4FRdKt0BiGTigKZRg9TE/annotation-Harf.png'}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'rotate': {'degrees': 11}, 'crop': {'min': 0, 'percent': 26}, 'shear': {'horizontal': 10, 'vertical': 10}}}, {'id': 'taa-ewhrv/text-recognation-svukt', 'type': 'classification', 'name': 'Text Recognation', 'created': 1746702986.492, 'updated': 1747213785.908, 'images': 609, 'unannotated': 0, 'annotation': 'Taha-Meryem-Elif-Berat-Ruhan', 'versions': 1, 'public': True, 'multilabel': False, 'license': 'CC BY 4.0', 'splits': {'export': 0, 'valid': 124, 'test': 60, 'train': 425}, 'colors': {'Unlabeled': '#C7FC00', 'taha': '#8622FF', 'Elif': '#FE0056', 'Meryem Ece AYYILDIZ': '#00FFCE', 'Berat': '#FF8000', 'rana': '#00B7EB'}, 'classes': {'Unlabeled': 0, 'taha': 133, 'Elif': 188, 'Meryem Ece AYYILDIZ': 99, 'Berat': 99, 'rana': 90}, 'icon': {'original': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/zhXOtUKU4N14vu2S9qwq/original.jpg', 'thumb': 'https://source.roboflow.com/os2sXLc0BeWecwvtUdmRFk6kdgt1/zhXOtUKU4N14vu2S9qwq/thumb.jpg', 'annotation': None}, 'preprocessing': {'auto-orient': True, 'resize': {'format': 'Stretch to', 'height': 640, 'width': 640}}, 'augmentation': {'image': {'versions': 3}, 'crop': {'min': 0, 'percent': 20}, 'exposure': {'percent': 10}, 'noise': {'percent': 0.1}, 'brightness': {'percent': 15, 'brighten': True, 'darken': True}}}]\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:33.979941Z",
     "start_time": "2025-05-15T09:10:29.370033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project = rf.workspace(\"taa-ewhrv\").project(\"line-detection-vylwz\")  # Workspace + Project slug\n",
    "model = project.version(1).model\n",
    "project2 = rf.workspace(\"taa-ewhrv\").project(\"text-recognation-svukt\") \n",
    "model2 = project2.version(1).model"
   ],
   "id": "c1dcef8f1ba137dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:36.779979Z",
     "start_time": "2025-05-15T09:10:33.997762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = model.predict(\"deneme.png\").json()\n",
    "\n",
    "print(prediction)"
   ],
   "id": "44440cddda294921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'x': 207, 'y': 730, 'width': 143, 'height': 69, 'confidence': 0.8106053471565247, 'class': 'Line', 'class_id': 0, 'detection_id': '21b4b763-3128-42dd-9f6e-eb1964556eb2', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 593, 'y': 472, 'width': 923, 'height': 82, 'confidence': 0.7945687770843506, 'class': 'Line', 'class_id': 0, 'detection_id': '3045c20b-d87e-4268-b5b8-04ebd1105625', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 637, 'y': 379, 'width': 1022, 'height': 82, 'confidence': 0.7915651798248291, 'class': 'Line', 'class_id': 0, 'detection_id': '56973b85-34de-4dc4-8419-9a7a685a3730', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 480, 'y': 817, 'width': 656, 'height': 85, 'confidence': 0.7896133065223694, 'class': 'Line', 'class_id': 0, 'detection_id': 'a6de3982-1027-40b7-bc22-1b46e38969be', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 612, 'y': 571, 'width': 944, 'height': 87, 'confidence': 0.7832475304603577, 'class': 'Line', 'class_id': 0, 'detection_id': '998cd2d5-dfe1-472b-a276-3c5535e3b06a', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 610, 'y': 970, 'width': 793, 'height': 95, 'confidence': 0.7740350961685181, 'class': 'Line', 'class_id': 0, 'detection_id': '5ba31365-f0fa-46ed-be7d-a27ebd607432', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 567, 'y': 286, 'width': 898, 'height': 93, 'confidence': 0.7727227807044983, 'class': 'Line', 'class_id': 0, 'detection_id': '5eaaf8ca-6bd1-4c78-b23c-042d096734e2', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}, {'x': 631, 'y': 657, 'width': 975, 'height': 90, 'confidence': 0.7656806707382202, 'class': 'Line', 'class_id': 0, 'detection_id': 'e5039ab0-117b-4c0f-bb25-cf7fc2b6dafc', 'image_path': 'deneme.png', 'prediction_type': 'ObjectDetectionModel'}], 'image': {'width': '1244', 'height': '1654'}}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:36.798706Z",
     "start_time": "2025-05-15T09:10:36.796277Z"
    }
   },
   "cell_type": "code",
   "source": "from PIL import Image",
   "id": "7eaeb17c084526f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:44.301763Z",
     "start_time": "2025-05-15T09:10:36.806467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sympy import false\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n"
   ],
   "id": "cb6affcf3a58d6f1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:44.310273Z",
     "start_time": "2025-05-15T09:10:44.308550Z"
    }
   },
   "cell_type": "code",
   "source": "import tempfile",
   "id": "c6369448260989bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:47.191983Z",
     "start_time": "2025-05-15T09:10:44.323686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten', use_fast=False)\n",
    "model_mic = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')"
   ],
   "id": "b07910653aca938a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:47.345913Z",
     "start_time": "2025-05-15T09:10:47.296387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "from flask_cors import CORS, cross_origin\n",
    "from io import BytesIO\n",
    "\n"
   ],
   "id": "e9d1ff1cab79d6e9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:47.434914Z",
     "start_time": "2025-05-15T09:10:47.431355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)"
   ],
   "id": "188dd764d12ddcb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x316315df0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:47.611400Z",
     "start_time": "2025-05-15T09:10:47.604985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@app.route(\"/predict\" , methods=['POST'])\n",
    "@cross_origin()\n",
    "def predict():\n",
    "    #!!!!!!!Ayarlanacak\n",
    "    image_file = request.files['image']\n",
    "    image0 = Image.open(image_file.stream).convert('RGB')\n",
    "    \n",
    "    prediction = model.predict(image0).json()\n",
    "    predictions = prediction[\"predictions\"]\n",
    "    image = Image.open(image0)\n",
    "    auth=\"\";\n",
    "    count =0;\n",
    "    results = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        count+=1;\n",
    "        x = pred['x']\n",
    "        y = pred['y']\n",
    "        w = pred['width']\n",
    "        h = pred['height']\n",
    "        \n",
    "        # (x, y) merkezden k√∂≈üe koordinatlarƒ±na √ßevir\n",
    "        left = int(x - w / 2)\n",
    "        top = int(y - h / 2)\n",
    "        right = int(x + w / 2)\n",
    "        bottom = int(y + h / 2)\n",
    "    \n",
    "        # G√∂rseli kƒ±rp\n",
    "        cropped = image.crop((left, top, right, bottom))\n",
    "        #cropped.show()\n",
    "        cropped = cropped.resize((cropped.width * 2, cropped.height * 2), Image.LANCZOS)\n",
    "        temp_cropped = cropped.convert(\"RGB\")\n",
    "        \n",
    "        #ge√ßici ram kayƒ±t\n",
    "        buffer = BytesIO()\n",
    "        temp_cropped.save(buffer, format=\"JPEG\")\n",
    "        buffer.seek(0)\n",
    "        prediction2 = model2.predict(buffer)\n",
    "        predictions2 = prediction2[0]\n",
    "        predictionDENEME = predictions2.json();\n",
    "        DENEME =predictionDENEME[\"top\"]\n",
    "        #print(DENEME)\n",
    "        #print(type(prediction2))\n",
    "        #print(dir(prediction2))\n",
    "        #print(type(prediction2[0]))\n",
    "        #print(dir(prediction2[0]))\n",
    "        auth = DENEME\n",
    "    \n",
    "    \n",
    "        #deneme_rgb=cropped.convert('RGB')\n",
    "        \n",
    "        image_array = np.array(cropped)\n",
    "        \n",
    "        \n",
    "        grayImage = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "        # Otsu Thresholding uygula\n",
    "        # Otsu, e≈üik deƒüerini otomatik olarak belirler\n",
    "        \n",
    "        adaptive_thresh = cv2.adaptiveThreshold(\n",
    "        grayImage, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        21,  # blockSize (daima tek sayƒ±, 11-15 arasƒ± genelde iyi)\n",
    "        4   # C deƒüeri (0 ya da negatif dene)\n",
    "    )\n",
    "        \n",
    "        ret, otsu_thresh = cv2.threshold(grayImage, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "        # Morfolojik i≈ülemler - harflerdeki kopmalarƒ± birle≈ütirmek i√ßin\n",
    "        # 1. Kapama (Closing) i≈ülemi - kopuk harfleri birle≈ütirir\n",
    "        kernel = np.ones((2, 2), np.uint8)  # Kernel boyutunu metninize g√∂re ayarlayabilirsiniz\n",
    "        closing = cv2.morphologyEx(otsu_thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "        # 2. Geni≈ületme (Dilation) i≈ülemi - harfleri daha kalƒ±n yapar, bo≈üluklarƒ± doldurur\n",
    "        dilation_kernel = np.ones((1, 1), np.uint8)\n",
    "        dilated_image = cv2.dilate(closing, dilation_kernel, iterations=1)\n",
    "    \n",
    "        # 3. Median filtreleme - g√ºr√ºlt√ºy√º azaltƒ±r\n",
    "        filtered_image = cv2.medianBlur(dilated_image, 3)\n",
    "    \n",
    "        # Numpy dizisini PIL Image'e d√∂n√º≈üt√ºr\n",
    "        threshold_image = Image.fromarray(otsu_thresh)\n",
    "        threshold_rgb = threshold_image.convert('RGB')\n",
    "        \n",
    "        last_image=Image.fromarray(adaptive_thresh)\n",
    "        last_img_rgb=last_image.convert('RGB')\n",
    "        #last_img_rgb.show()\n",
    "        \n",
    "        # G√∂r√ºnt√ºy√º g√∂ster\n",
    "        #threshold_rgb.show()\n",
    "        #deneme_rgb.show()\n",
    "    \n",
    "        # Modelle i≈ülem i√ßin g√∂r√ºnt√ºy√º hazƒ±rla\n",
    "        pixel_values = processor(images=threshold_rgb, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "        # Tahmin yap\n",
    "        generated_ids = model_mic.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        #print(f\"{count}. satƒ±r yazarƒ± : {auth}\")\n",
    "    \n",
    "        #print(\"\\nüñãÔ∏è Tanƒ±nan Metin:\\n\")\n",
    "        #print(generated_text)\n",
    "        \n",
    "        results.append({\n",
    "            \"line_number\": count,\n",
    "            \"predicted_text\": generated_text,\n",
    "            \"predicted_author\": auth\n",
    "        })\n",
    "        \n",
    "        \n",
    "    print(results)\n",
    "    return jsonify(results)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/hello\" , methods=['GET'])    \n",
    "@cross_origin()\n",
    "def hello() :\n",
    "    return \"Hello World!\"\n",
    "\n",
    "def run_app():\n",
    "    print(\"uyg ba≈üaldƒ± mƒ±\")\n",
    "    app.run(port=5001)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n",
    "    "
   ],
   "id": "432d800603a64ad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uyg ba≈üaldƒ± mƒ±\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:10:47.683120Z",
     "start_time": "2025-05-15T09:10:47.681748Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8650fc3e0d6498d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
